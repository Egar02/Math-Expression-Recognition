{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4d801906",
      "metadata": {
        "id": "4d801906"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7c985bff",
      "metadata": {
        "id": "7c985bff"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "448bf355",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "448bf355",
        "outputId": "381a40e8-9e77-49fe-e7b4-75157313c29d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchinfo'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c05c7ab28e6d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f75301ae",
      "metadata": {
        "id": "f75301ae"
      },
      "source": [
        "# Работа с данными"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051e6293",
      "metadata": {
        "id": "051e6293"
      },
      "source": [
        "## Считывание данных"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d618927",
      "metadata": {
        "id": "6d618927"
      },
      "source": [
        "### Считываем изображения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "035b7dd7",
      "metadata": {
        "id": "035b7dd7"
      },
      "outputs": [],
      "source": [
        "img_dir = r\"./Dataset/formula_images/\"\n",
        "img_cropped_dir = r\"./Dataset/formula_images_cropped/\"\n",
        "formulas_dir = r\"./Dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2825215c",
      "metadata": {
        "id": "2825215c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "images = os.listdir(img_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39dabbae",
      "metadata": {
        "id": "39dabbae"
      },
      "outputs": [],
      "source": [
        "images_full_path = [img_dir + img for img in images]\n",
        "images_cropped_full_path = [img_cropped_dir + img for img in images]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d546753",
      "metadata": {
        "id": "9d546753"
      },
      "source": [
        "### Делаем crop изображений и сохраняем в другую директорию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c15b1e",
      "metadata": {
        "id": "77c15b1e"
      },
      "outputs": [],
      "source": [
        "x0 = 300\n",
        "x1 = 600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7155216",
      "metadata": {
        "id": "d7155216"
      },
      "outputs": [],
      "source": [
        "for img_path, img_cropped_path in zip(images_full_path, images_cropped_full_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    cv2.imwrite(img_cropped_path, img[x0:x1, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4251f6cd",
      "metadata": {
        "id": "4251f6cd"
      },
      "source": [
        "### Считываем формулы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c62b59d",
      "metadata": {
        "id": "7c62b59d"
      },
      "outputs": [],
      "source": [
        "formulas = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3464d7c5",
      "metadata": {
        "id": "3464d7c5"
      },
      "outputs": [],
      "source": [
        "with open(formulas_dir + \"im2latex_formulas.lst\", newline=\"\\n\", encoding='latin-1') as file:\n",
        "    for idx, line in enumerate(file, start=0):\n",
        "        formulas[idx] = line.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ed6aaa",
      "metadata": {
        "id": "82ed6aaa"
      },
      "outputs": [],
      "source": [
        "all_characters = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1b9536e",
      "metadata": {
        "id": "b1b9536e"
      },
      "outputs": [],
      "source": [
        "with open(formulas_dir + \"SYMLIST.txt\", newline='\\n') as file:\n",
        "    for idx, line in enumerate(file, start=0):\n",
        "        all_characters.add(line[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4224f43",
      "metadata": {
        "id": "e4224f43",
        "outputId": "d98f23f8-c08a-439b-c5df-9663f29c4a14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16482"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79141b86",
      "metadata": {
        "id": "79141b86"
      },
      "outputs": [],
      "source": [
        "characters = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897185f1",
      "metadata": {
        "id": "897185f1",
        "outputId": "64d7be77-2fc7-4eeb-c428-be853240fea5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 104546/104546 [03:12<00:00, 542.19it/s]\n"
          ]
        }
      ],
      "source": [
        "for idx, formula in tqdm(formulas.items()):\n",
        "    for lat_char in all_characters:\n",
        "        if lat_char in formula:\n",
        "            characters.add(lat_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac5629f",
      "metadata": {
        "id": "eac5629f",
        "outputId": "2df4b2a4-f31c-43c4-ca53-9967df599f1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d63a45",
      "metadata": {
        "id": "74d63a45"
      },
      "outputs": [],
      "source": [
        "with open(formulas_dir + \"dict.txt\", newline='\\n') as file:\n",
        "    for idx, line in enumerate(file, start=0):\n",
        "        characters.add(line[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3bbb578",
      "metadata": {
        "id": "c3bbb578",
        "outputId": "ce3cec4b-975e-4ce1-dd22-7833f01d756e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "537"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1c721b",
      "metadata": {
        "id": "aa1c721b"
      },
      "outputs": [],
      "source": [
        "characters_copy = characters.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc1cba0d",
      "metadata": {
        "id": "dc1cba0d"
      },
      "outputs": [],
      "source": [
        "for character in characters_copy:\n",
        "    if ('{' in character or '}' in character) and len(character) > 1:\n",
        "        characters.remove(character)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0669a6",
      "metadata": {
        "id": "0f0669a6"
      },
      "outputs": [],
      "source": [
        "characters_copy = characters.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e02a5c8",
      "metadata": {
        "id": "2e02a5c8"
      },
      "outputs": [],
      "source": [
        "for character in characters_copy:\n",
        "    characters.remove(character)\n",
        "    characters.add(character.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42950c31",
      "metadata": {
        "id": "42950c31"
      },
      "outputs": [],
      "source": [
        "greek = (\"\\\\Alpha\", \"\\\\Beta\", \"\\\\Gamma\", \"\\\\Delta\", \"\\\\Epsilon\", \"\\\\Zeta\", \"\\\\Eta\", \"\\\\Theta\", \"\\\\Iota\", \"\\\\Kappa\", \"\\\\Lambda\", \"\\\\Mu\",\n",
        "         \"\\\\Nu\", \"\\\\Xi\", \"\\\\Omicron\", \"\\\\Pi\", \"\\\\Rho\", \"\\\\Sigma\", \"\\\\Tau\", \"\\\\Upsilon\", \"\\\\Phi\", \"\\\\Chi\", \"\\\\Psi\", \"\\\\Omega\")\n",
        "eng = (\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94bf8510",
      "metadata": {
        "id": "94bf8510"
      },
      "outputs": [],
      "source": [
        "characters.update(greek)\n",
        "characters.update(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367f121c",
      "metadata": {
        "id": "367f121c",
        "outputId": "bdcb365f-d9a8-4fca-f5cc-aadaf9d087d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "435"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3289d72",
      "metadata": {
        "id": "c3289d72"
      },
      "outputs": [],
      "source": [
        "characters.add(\":\")\n",
        "characters.add(\";\")\n",
        "characters.add(\"~\")\n",
        "characters.add(\"&\")\n",
        "characters.add(\"%\")\n",
        "characters.add(\"$\")\n",
        "characters.add(\"@\")\n",
        "characters.add('\"')\n",
        "characters.add('`')\n",
        "characters.add('?')\n",
        "characters.add('#')\n",
        "characters.add(' ')\n",
        "\n",
        "characters.add('ç')\n",
        "characters.add('¥')\n",
        "characters.add('')\n",
        "characters.add('§')\n",
        "characters.add('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9fe30a4",
      "metadata": {
        "id": "e9fe30a4"
      },
      "outputs": [],
      "source": [
        "vocab = {}\n",
        "vocab_r = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb03873",
      "metadata": {
        "id": "1fb03873"
      },
      "outputs": [],
      "source": [
        "for idx, character in enumerate(characters):\n",
        "    vocab[character] = idx\n",
        "    vocab_r[idx] = character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c6374e",
      "metadata": {
        "id": "55c6374e",
        "outputId": "6fe89f50-a277-4984-af6d-c9b762aa7089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "452"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186444a1",
      "metadata": {
        "id": "186444a1"
      },
      "outputs": [],
      "source": [
        "vocab_dict = {\"idx\": list(vocab.values()), \"character\": list(vocab.keys())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753fa502",
      "metadata": {
        "id": "753fa502"
      },
      "outputs": [],
      "source": [
        "vocab_df = pd.DataFrame.from_dict(vocab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8347104a",
      "metadata": {
        "id": "8347104a"
      },
      "outputs": [],
      "source": [
        "vocab_df.to_csv(\"vocab.csv\", sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde737ec",
      "metadata": {
        "id": "cde737ec"
      },
      "outputs": [],
      "source": [
        "def tok_to_str(tok_list, vocab_r):\n",
        "    st = \"\"\n",
        "    for tok in tok_list:\n",
        "        st = st + vocab_r[tok]\n",
        "\n",
        "    return st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf35621",
      "metadata": {
        "id": "9bf35621",
        "outputId": "f3a4f1ec-09cc-4d18-9a91-9a072864dbff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'_' in characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d026f31",
      "metadata": {
        "id": "4d026f31"
      },
      "outputs": [],
      "source": [
        "formulas = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3359e5",
      "metadata": {
        "id": "4c3359e5"
      },
      "outputs": [],
      "source": [
        "with open(formulas_dir + \"im2latex_formulas.lst\", newline=\"\\n\", encoding='latin-1') as file:\n",
        "    for idx, line in enumerate(file, start=0):\n",
        "        formulas[idx] = line.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba2f123e",
      "metadata": {
        "id": "ba2f123e",
        "outputId": "b49bd2f6-7588-4861-ad2c-19974647fe93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 7252/104546 [00:23<05:12, 311.73it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(formulas\u001b[38;5;241m.\u001b[39mitems()))):\n\u001b[1;32m----> 2\u001b[0m     tok_list \u001b[38;5;241m=\u001b[39m encode_truth(formulas[idx], vocab)\n",
            "Cell \u001b[1;32mIn[22], line 10\u001b[0m, in \u001b[0;36mencode_truth\u001b[1;34m(truth, token_to_id)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(remaining_truth) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m         matching_starts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m             [i, \u001b[38;5;28mlen\u001b[39m(tok)]\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m tok, i \u001b[38;5;129;01min\u001b[39;00m token_to_id\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m remaining_truth\u001b[38;5;241m.\u001b[39mstartswith(tok)\n\u001b[0;32m     14\u001b[0m         ]\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# Take the longest match\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         index, tok_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(matching_starts, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m match: match[\u001b[38;5;241m1\u001b[39m])\n",
            "Cell \u001b[1;32mIn[22], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(remaining_truth) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m         matching_starts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m             [i, \u001b[38;5;28mlen\u001b[39m(tok)]\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m tok, i \u001b[38;5;129;01min\u001b[39;00m token_to_id\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m remaining_truth\u001b[38;5;241m.\u001b[39mstartswith(tok)\n\u001b[0;32m     14\u001b[0m         ]\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# Take the longest match\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         index, tok_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(matching_starts, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m match: match[\u001b[38;5;241m1\u001b[39m])\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for idx in tqdm(range(len(formulas.items()))):\n",
        "    tok_list = encode_truth(formulas[idx], vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7226547d",
      "metadata": {
        "id": "7226547d",
        "outputId": "a32f7c21-521f-47fe-e605-e7d56baf3887"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\\\label{oneterm}{\\\\cal Z}^{(2)}_1 = \\\\int d^{10} A_3 (J^3)^{16} e^{-V(|A_3|)}'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formulas[100000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a69341",
      "metadata": {
        "id": "28a69341",
        "outputId": "435c1114-5421-4f01-b146-6983c256e92d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "104546"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(formulas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "babfb3fd",
      "metadata": {
        "id": "babfb3fd",
        "outputId": "c174d0f0-0f3f-4a47-8837-c94463fbfdb9"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'<sos>'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tok_to_str(tok_list, vocab_r)\n",
            "Cell \u001b[1;32mIn[43], line 4\u001b[0m, in \u001b[0;36mtok_to_str\u001b[1;34m(tok_list, vocab_r)\u001b[0m\n\u001b[0;32m      2\u001b[0m st \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m tok_list:\n\u001b[1;32m----> 4\u001b[0m     st \u001b[38;5;241m=\u001b[39m st \u001b[38;5;241m+\u001b[39m vocab_r[tok]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m st\n",
            "\u001b[1;31mKeyError\u001b[0m: '<sos>'"
          ]
        }
      ],
      "source": [
        "tok_to_str(tok_list, vocab_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c20ca92",
      "metadata": {
        "id": "6c20ca92"
      },
      "source": [
        "### Создаём словарь {путь к изображению: формула}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b51a4b7",
      "metadata": {
        "id": "5b51a4b7"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(formulas_dir + \"im2latex_train.lst\", delimiter=\" \", header=None)\n",
        "train_data.columns = ['formula_idx', 'img_path', 'type']\n",
        "train_data[\"img_path\"] = train_data[\"img_path\"].apply(lambda x: img_cropped_dir + x + \".png\")\n",
        "\n",
        "train_dataset_dict = {}\n",
        "for index, row in train_data.iterrows():\n",
        "    train_dataset_dict[row[\"img_path\"]] = formulas[row[\"formula_idx\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce21865",
      "metadata": {
        "id": "6ce21865"
      },
      "outputs": [],
      "source": [
        "val_data = pd.read_csv(formulas_dir + \"im2latex_validate.lst\", delimiter=\" \", header=None)\n",
        "val_data.columns = ['formula_idx', 'img_path', 'type']\n",
        "val_data[\"img_path\"] = val_data[\"img_path\"].apply(lambda x: img_cropped_dir + x + \".png\")\n",
        "\n",
        "val_dataset_dict = {}\n",
        "for index, row in val_data.iterrows():\n",
        "    val_dataset_dict[row[\"img_path\"]] = formulas[row[\"formula_idx\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef749b2d",
      "metadata": {
        "id": "ef749b2d"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(formulas_dir + \"im2latex_test.lst\", delimiter=\" \", header=None)\n",
        "test_data.columns = ['formula_idx', 'img_path', 'type']\n",
        "test_data[\"img_path\"] = test_data[\"img_path\"].apply(lambda x: img_cropped_dir + x + \".png\")\n",
        "\n",
        "test_dataset_dict = {}\n",
        "for index, row in test_data.iterrows():\n",
        "    test_dataset_dict[row[\"img_path\"]] = formulas[row[\"formula_idx\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ebce05",
      "metadata": {
        "id": "a5ebce05"
      },
      "source": [
        "## Добавление аугментаций"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49aa867",
      "metadata": {
        "id": "b49aa867"
      },
      "source": [
        "### Добавляем только аугментации, не искажающие сами формулы: поворот, смещение, масштабирование, небольшой blur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1b9041",
      "metadata": {
        "id": "ba1b9041"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c1bcaa",
      "metadata": {
        "id": "96c1bcaa"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.75, rotate_limit=20, p=1.0),\n",
        "    A.Blur(blur_limit=3)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673aacc5",
      "metadata": {
        "id": "673aacc5"
      },
      "outputs": [],
      "source": [
        "for i in range(int(0.5 * len(train_dataset_dict))):\n",
        "    img_path, formula = random.choice(list(train_dataset_dict.items()))\n",
        "\n",
        "    aug_name = \"a\" + str(i) + \".png\"\n",
        "\n",
        "    aug_path = img_cropped_dir + aug_name\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    aug_img = transform(image=img)['image']\n",
        "\n",
        "    cv2.imwrite(aug_path, aug_img)\n",
        "\n",
        "    train_dataset_dict[aug_path] = formula"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb2b13b",
      "metadata": {
        "id": "cbb2b13b"
      },
      "source": [
        "### Сохраняем получившиеся словари в csv-файлы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5b10b7",
      "metadata": {
        "id": "3c5b10b7",
        "outputId": "997a27a4-fa75-4621-aa90-780d4d7ca529"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_dataset_dict' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf_train \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(train_dataset_dict\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformula\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(train_dataset_dict\u001b[38;5;241m.\u001b[39mvalues())}\n\u001b[0;32m      2\u001b[0m tf_val \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(val_dataset_dict\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformula\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(val_dataset_dict\u001b[38;5;241m.\u001b[39mvalues())}\n\u001b[0;32m      3\u001b[0m tf_test \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(test_dataset_dict\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformula\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(test_dataset_dict\u001b[38;5;241m.\u001b[39mvalues())}\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_dataset_dict' is not defined"
          ]
        }
      ],
      "source": [
        "tf_train = {\"img_path\": list(train_dataset_dict.keys()), \"formula\": list(train_dataset_dict.values())}\n",
        "tf_val = {\"img_path\": list(val_dataset_dict.keys()), \"formula\": list(val_dataset_dict.values())}\n",
        "tf_test = {\"img_path\": list(test_dataset_dict.keys()), \"formula\": list(test_dataset_dict.values())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7133b643",
      "metadata": {
        "id": "7133b643"
      },
      "outputs": [],
      "source": [
        "train_dataset_df = pd.DataFrame.from_dict(tf_train)\n",
        "val_dataset_df = pd.DataFrame.from_dict(tf_val)\n",
        "test_dataset_df = pd.DataFrame.from_dict(tf_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9e6be6",
      "metadata": {
        "id": "5b9e6be6"
      },
      "outputs": [],
      "source": [
        "train_dataset_df.to_csv(formulas_dir + \"train_dict.csv\", sep=\"к\")\n",
        "val_dataset_df.to_csv(formulas_dir + \"val_dict.csv\", sep=\"к\")\n",
        "test_dataset_df.to_csv(formulas_dir + \"test_dict.csv\", sep=\"к\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66631efb",
      "metadata": {
        "id": "66631efb"
      },
      "source": [
        "## Создание Dataset'ов и Dataloader'ов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44aa76a",
      "metadata": {
        "id": "c44aa76a"
      },
      "source": [
        "### Считываем словари {путь к изображению: формула}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9eb7e352",
      "metadata": {
        "id": "9eb7e352"
      },
      "outputs": [],
      "source": [
        "START = \"<SOS>\"\n",
        "END = \"<EOS>\"\n",
        "PAD = \"<PAD>\"\n",
        "SPECIAL_TOKENS = [START, END, PAD]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c5635105",
      "metadata": {
        "id": "c5635105"
      },
      "outputs": [],
      "source": [
        "img_dir = r\"./Dataset/formula_images/\"\n",
        "img_cropped_dir = r\"./Dataset/formula_images_cropped/\"\n",
        "formulas_dir = r\"./Dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1c78aa66",
      "metadata": {
        "id": "1c78aa66"
      },
      "outputs": [],
      "source": [
        "def load_vocab(tokens_file):\n",
        "    with open(tokens_file, \"r\") as fd:\n",
        "        tokens = []\n",
        "        df = pd.read_csv(tokens_file, sep='\\t')\n",
        "        for index, row in df.iterrows():\n",
        "            tokens.append(row[\"character\"])\n",
        "        tokens.extend(SPECIAL_TOKENS)\n",
        "        token_to_id = {tok: i for i, tok in enumerate(tokens)}\n",
        "        id_to_token = {i: tok for i, tok in enumerate(tokens)}\n",
        "        return token_to_id, id_to_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e6331148",
      "metadata": {
        "id": "e6331148"
      },
      "outputs": [],
      "source": [
        "token_to_id, id_to_token = load_vocab(\"vocab.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5c221a5a",
      "metadata": {
        "id": "5c221a5a"
      },
      "outputs": [],
      "source": [
        "def encode_truth(truth, token_to_id):\n",
        "\n",
        "    truth_tokens = []\n",
        "    remaining_truth = truth\n",
        "    while len(remaining_truth) > 0:\n",
        "        try:\n",
        "            matching_starts = [\n",
        "                [i, len(tok)]\n",
        "                for tok, i in token_to_id.items()\n",
        "                if remaining_truth.startswith(tok)\n",
        "            ]\n",
        "            # Take the longest match\n",
        "            index, tok_len = max(matching_starts, key=lambda match: match[1])\n",
        "            truth_tokens.append(index)\n",
        "            remaining_truth = remaining_truth[tok_len:].lstrip()\n",
        "        except ValueError:\n",
        "            print(truth)\n",
        "            raise Exception(\"Truth contains unknown token\")\n",
        "\n",
        "    return truth_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ec0c65ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec0c65ad",
        "outputId": "bc103d93-ff70-433d-9a9b-dfc3b6c2b07f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-28d685458f46>:1: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
            "  train_dataset_df = pd.read_csv(formulas_dir + \"train_dict.csv\", delimiter=\"к\")\n",
            "127521it [09:15, 229.65it/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset_df = pd.read_csv(formulas_dir + \"train_dict.csv\", delimiter=\"к\")\n",
        "train_dataset_dict = {}\n",
        "for index, row in tqdm(train_dataset_df.iterrows()):\n",
        "    train_dataset_dict[row[\"img_path\"]] = {\"truth\": row[\"formula\"],\n",
        "                                           \"encoded\": [token_to_id[START], *encode_truth(row[\"formula\"], token_to_id), token_to_id[END]]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f428d0f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f428d0f7",
        "outputId": "a818be59-ed13-4c9f-e47b-6fe239932c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-cb981cf9c809>:1: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
            "  val_dataset_df = pd.read_csv(formulas_dir + \"val_dict.csv\", delimiter=\"к\")\n",
            "9317it [00:38, 241.94it/s]\n"
          ]
        }
      ],
      "source": [
        "val_dataset_df = pd.read_csv(formulas_dir + \"val_dict.csv\", delimiter=\"к\")\n",
        "val_dataset_dict = {}\n",
        "for index, row in tqdm(val_dataset_df.iterrows()):\n",
        "    val_dataset_dict[row[\"img_path\"]] = {\"truth\": row[\"formula\"],\n",
        "                                         \"encoded\": [token_to_id[START], *encode_truth(row[\"formula\"], token_to_id), token_to_id[END]]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1f6525c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f6525c7",
        "outputId": "6aec0ae4-f21c-4b60-f1ed-f7a059992d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-ec99b82a8926>:1: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
            "  test_dataset_df = pd.read_csv(formulas_dir + \"test_dict.csv\", delimiter=\"к\")\n",
            "10353it [00:45, 226.40it/s]\n"
          ]
        }
      ],
      "source": [
        "test_dataset_df = pd.read_csv(formulas_dir + \"test_dict.csv\", delimiter=\"к\")\n",
        "test_dataset_dict = {}\n",
        "for index, row in tqdm(test_dataset_df.iterrows()):\n",
        "    test_dataset_dict[row[\"img_path\"]] = {\"truth\": row[\"formula\"],\n",
        "                                          \"encoded\": [token_to_id[START], *encode_truth(row[\"formula\"], token_to_id), token_to_id[END]]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "80561f88",
      "metadata": {
        "id": "80561f88"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "\n",
        "for formula in train_dataset_dict.values():\n",
        "    if len(formula[\"encoded\"]) > max_len:\n",
        "        max_len = len(formula[\"encoded\"])\n",
        "\n",
        "for formula in val_dataset_dict.values():\n",
        "    if len(formula[\"encoded\"]) > max_len:\n",
        "        max_len = len(formula[\"encoded\"])\n",
        "\n",
        "for formula in test_dataset_dict.values():\n",
        "    if len(formula[\"encoded\"]) > max_len:\n",
        "        max_len = len(formula[\"encoded\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d0c51f",
      "metadata": {
        "id": "98d0c51f",
        "outputId": "0e6ded9b-8fc9-407c-d2c3-3d8eaa9b3545"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "874"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcbbb98b",
      "metadata": {
        "id": "bcbbb98b"
      },
      "source": [
        "### Класс Dataset: раздаём инвертированное изображение в виде тензора и представление формулы в виде LaTex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAzyaB4gW0kk",
        "outputId": "ea6055f5-73a5-4de7-db1f-5a4beb0781ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "id": "MAzyaB4gW0kk"
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "kmmZ7zJBXVu4"
      },
      "id": "kmmZ7zJBXVu4",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_dir = \"/content/drive/MyDrive/Dataset/formula_images_cropped.zip\"\n",
        "z = zipfile.ZipFile(zip_dir, \"r\")"
      ],
      "metadata": {
        "id": "-O-89kT6XXw7"
      },
      "id": "-O-89kT6XXw7",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_dir = \"./Dataset\"\n",
        "z.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "GTYo7GcDd475"
      },
      "id": "GTYo7GcDd475",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f0a75961",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "f0a75961",
        "outputId": "47b6aeda-4914-4adc-c861-bd5b0464ad11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1654x300>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Image.open(img_cropped_dir + \"5f8f6eac29.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1f2f45c5",
      "metadata": {
        "id": "1f2f45c5"
      },
      "outputs": [],
      "source": [
        "class MERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_formula_dict, transforms=None):\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.imgs_path = list(img_formula_dict.keys())\n",
        "\n",
        "        self.formulas = list(img_formula_dict.values())\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images\n",
        "        img_path = self.imgs_path[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "            img = transforms.functional.invert(img)\n",
        "\n",
        "        formula = self.formulas[idx][\"truth\"]\n",
        "        formula_encoded = self.formulas[idx][\"encoded\"]\n",
        "\n",
        "        return img, formula, formula_encoded\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "833221d4",
      "metadata": {
        "id": "833221d4"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    batch.sort(key=lambda x: len(x[2]), reverse=True)\n",
        "\n",
        "    imgs, truths, labels = zip(*batch)\n",
        "\n",
        "    max_len = max([len(label) for label in labels])\n",
        "    # Padding with -1, will later be replaced with the PAD token\n",
        "    padded_labels = [\n",
        "        label + (max_len - len(label)) * [-1]\n",
        "        for label in labels\n",
        "    ]\n",
        "\n",
        "    return torch.concat(imgs, dim=0).unsqueeze(1), truths, torch.tensor(padded_labels, dtype=torch.long)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "70d487e5",
      "metadata": {
        "id": "70d487e5"
      },
      "outputs": [],
      "source": [
        "transforms_1 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((30, 100)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c4f94310",
      "metadata": {
        "id": "c4f94310"
      },
      "outputs": [],
      "source": [
        "train_dataset = MERDataset(train_dataset_dict, transforms=transforms_1)\n",
        "val_dataset = MERDataset(val_dataset_dict, transforms=transforms_1)\n",
        "test_dataset = MERDataset(test_dataset_dict, transforms=transforms_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34aef32b",
      "metadata": {
        "id": "34aef32b"
      },
      "source": [
        "# Создание архитектуры нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6d5d419a",
      "metadata": {
        "id": "6d5d419a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "680969fa",
      "metadata": {
        "id": "680969fa"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ddfafc7f",
      "metadata": {
        "id": "ddfafc7f"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1b41bd5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1b41bd5a",
        "outputId": "7ad744ec-da0b-4a70-c51f-11c2a372d146"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5fa0b8f",
      "metadata": {
        "id": "a5fa0b8f"
      },
      "source": [
        "## Encoder - свёрточная нейронная сеть"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d51aef03",
      "metadata": {
        "id": "d51aef03"
      },
      "outputs": [],
      "source": [
        "# Number of bottlenecks\n",
        "num_bn = 3\n",
        "depth = 16\n",
        "multi_block_depth = depth // 2\n",
        "growth_rate = 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "5c805c82",
      "metadata": {
        "id": "5c805c82"
      },
      "outputs": [],
      "source": [
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, input_size, growth_rate, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            growth_rate (int): Number of new features being added. That is the ouput\n",
        "                size of the last convolutional layer.\n",
        "            dropout_rate (float, optional): Probability of dropout [Default: 0.2]\n",
        "        \"\"\"\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        inter_size = num_bn * growth_rate\n",
        "        self.norm1 = nn.BatchNorm2d(input_size)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            input_size, inter_size, kernel_size=1, stride=1, bias=False\n",
        "        )\n",
        "        self.norm2 = nn.BatchNorm2d(inter_size)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            inter_size, growth_rate, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.relu(self.norm1(x)))\n",
        "        out = self.conv2(self.relu(self.norm2(out)))\n",
        "        out = self.dropout(out)\n",
        "        return torch.cat([x, out], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2cd811e2",
      "metadata": {
        "id": "2cd811e2"
      },
      "outputs": [],
      "source": [
        "class TransitionBlock(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            output_size (int): Number of channels of the output\n",
        "        \"\"\"\n",
        "        super(TransitionBlock, self).__init__()\n",
        "        self.norm = nn.BatchNorm2d(input_size)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(\n",
        "            input_size, output_size, kernel_size=1, stride=1, bias=False\n",
        "        )\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.norm(x)))\n",
        "        return self.pool(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2a705dbb",
      "metadata": {
        "id": "2a705dbb"
      },
      "outputs": [],
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Dense block\n",
        "\n",
        "    A dense block stacks several bottleneck blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, growth_rate, depth, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            growth_rate (int): Number of new features being added per bottleneck block\n",
        "            depth (int): Number of bottleneck blocks\n",
        "            dropout_rate (float, optional): Probability of dropout [Default: 0.2]\n",
        "        \"\"\"\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layers = [\n",
        "            BottleneckBlock(\n",
        "                input_size + i * growth_rate, growth_rate, dropout_rate=dropout_rate\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ]\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "790262b4",
      "metadata": {
        "id": "790262b4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, img_channels=1, num_in_features=48, dropout_rate=0.2, checkpoint=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_channels (int, optional): Number of channels of the images [Default: 1]\n",
        "            num_in_features (int, optional): Number of channels that are created from\n",
        "                the input to feed to the first dense block [Default: 48]\n",
        "            dropout_rate (float, optional): Probability of dropout [Default: 0.2]\n",
        "            checkpoint (dict, optional): State dictionary to be loaded\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv0 = nn.Conv2d(\n",
        "            img_channels,\n",
        "            num_in_features,\n",
        "            kernel_size=7,\n",
        "            stride=2,\n",
        "            padding=3,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.norm0 = nn.BatchNorm2d(num_in_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        num_features = num_in_features\n",
        "        self.block1 = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        num_features = num_features + depth * growth_rate\n",
        "        self.trans1 = TransitionBlock(num_features, num_features // 2)\n",
        "        num_features = num_features // 2\n",
        "        self.block2 = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "\n",
        "        num_features = num_features + depth * growth_rate\n",
        "        self.trans2_norm = nn.BatchNorm2d(num_features)\n",
        "        self.trans2_relu = nn.ReLU(inplace=True)\n",
        "        self.trans2_conv = nn.Conv2d(\n",
        "            num_features, num_features // 2, kernel_size=1, stride=1, bias=False\n",
        "        )\n",
        "        self.trans2_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.multi_block = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=multi_block_depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        num_features = num_features // 2\n",
        "        self.block3 = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            self.load_state_dict(checkpoint)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv0(x)\n",
        "        out = self.relu(self.norm0(out))\n",
        "        out = self.max_pool(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.trans1(out)\n",
        "        out = self.block2(out)\n",
        "        out_before_trans2 = self.trans2_relu(self.trans2_norm(out))\n",
        "        out_A = self.trans2_conv(out_before_trans2)\n",
        "        out_A = self.trans2_pool(out_A)\n",
        "        out_A = self.block3(out_A)\n",
        "\n",
        "        return out_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f835ee1e",
      "metadata": {
        "id": "f835ee1e"
      },
      "outputs": [],
      "source": [
        "enc = Encoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5cedda08",
      "metadata": {
        "id": "5cedda08"
      },
      "outputs": [],
      "source": [
        "enc.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "10b8bdb0",
      "metadata": {
        "id": "10b8bdb0"
      },
      "outputs": [],
      "source": [
        "sample = torch.zeros(10, 1, 120, 660, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "67941556",
      "metadata": {
        "id": "67941556"
      },
      "outputs": [],
      "source": [
        "low_res = enc(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6015e2b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6015e2b5",
        "outputId": "c17b840b-cfad-4527-efe2-08bde3204a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 684, 7, 41])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "low_res_shape = low_res.shape\n",
        "low_res_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7eebc9",
      "metadata": {
        "id": "4c7eebc9"
      },
      "source": [
        "## Decoder - рекуррентная нейронная сеть с механизмом внимания"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "04a6c1d0",
      "metadata": {
        "id": "04a6c1d0"
      },
      "outputs": [],
      "source": [
        "n = 256\n",
        "n_prime = 512\n",
        "decoder_conv_filters = 256\n",
        "gru_hidden_size = 256\n",
        "embedding_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "8dc77ece",
      "metadata": {
        "id": "8dc77ece"
      },
      "outputs": [],
      "source": [
        "class CoverageAttention(nn.Module):\n",
        "    # input_size = C\n",
        "    # output_size = q\n",
        "    # attn_size = L = H * W\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        output_size,\n",
        "        attn_size,\n",
        "        kernel_size,\n",
        "        padding=0,\n",
        "        device=device,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            output_size (int): Number of channels of the coverage\n",
        "            attn_size (int): Length of the annotation vector\n",
        "            kernel_size (int): Kernel size of the 1D convolutional layer\n",
        "            padding (int, optional): Padding of the 1D convolutional layer [Default: 0]\n",
        "            device (torch.device, optional): Device for the tensors\n",
        "        \"\"\"\n",
        "        super(CoverageAttention, self).__init__()\n",
        "        self.alpha = None\n",
        "        self.conv = nn.Conv2d(1, output_size, kernel_size=kernel_size, padding=padding)\n",
        "        self.U_a = nn.Parameter(torch.empty((n_prime, input_size)))\n",
        "        self.U_f = nn.Parameter(torch.empty((n_prime, output_size)))\n",
        "        self.nu_attn = nn.Parameter(torch.empty(n_prime))\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.attn_size = attn_size\n",
        "        self.device = device\n",
        "        nn.init.xavier_normal_(self.U_a)\n",
        "        nn.init.xavier_normal_(self.U_f)\n",
        "        # Xavier requires at least a 2D tensor.\n",
        "        nn.init.xavier_normal_(self.nu_attn.unsqueeze(0))\n",
        "\n",
        "    def reset_alpha(self, batch_size):\n",
        "        self.alpha = torch.zeros((batch_size, 1, self.attn_size), device=self.device)\n",
        "\n",
        "    def forward(self, x, u_pred):\n",
        "        batch_size = x.size(0)\n",
        "        if self.alpha is None:\n",
        "            self.reset_alpha(batch_size)\n",
        "        # Change the dimensions to make it possible to apply a 2D convolution\n",
        "        # From: (batch_size x L)\n",
        "        # To: (batch_size x H x W)\n",
        "        alpha_sum = self.alpha.sum(1).view(batch_size, x.size(2), x.size(3))\n",
        "        conv_out = self.conv(alpha_sum.unsqueeze(1))\n",
        "        # Change dimensions back\n",
        "        # From: (batch_size x output_size x H x W)\n",
        "        # To: (batch_size x output_size x L)\n",
        "        conv_out = conv_out.view(batch_size, self.output_size, -1)\n",
        "        # Change the dimensions\n",
        "        # From: (batch_size x C x H x W)\n",
        "        # To: (batch_size x C x L)\n",
        "        a = x.view(batch_size, x.size(1), -1)\n",
        "        u_a = torch.matmul(self.U_a, a)\n",
        "        u_f = torch.matmul(self.U_f, conv_out)\n",
        "        # u_pred is expanded from (batch_size x n_prime)\n",
        "        # to (batch_size x n_prime x L) because there are L components to which\n",
        "        # the same u_pred is added.\n",
        "        u_pred_expanded = u_pred.unsqueeze(2).expand_as(u_a)\n",
        "        tan_res = torch.tanh(u_pred_expanded + u_a + u_f)\n",
        "        e_t = torch.matmul(self.nu_attn, tan_res)\n",
        "        alpha_t = torch.softmax(e_t, dim=1)\n",
        "        self.alpha = torch.cat((self.alpha, alpha_t.detach().unsqueeze(1)), dim=1)\n",
        "        # alpha_t: (batch_size x L)\n",
        "        # a: (batch_size x C x L) but need (C x batch_size x L) for\n",
        "        # element-wise multiplication. So transpose them.\n",
        "        cA_t_L = alpha_t * a.transpose(0, 1)\n",
        "        # Transpose back\n",
        "        return cA_t_L.transpose(0, 1).sum(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "388f9b26",
      "metadata": {
        "id": "388f9b26"
      },
      "outputs": [],
      "source": [
        "class Maxout(nn.Module):\n",
        "    def __init__(self, pool_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pool_size (int): Number of elements per pool\n",
        "        \"\"\"\n",
        "        super(Maxout, self).__init__()\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        [*shape, last] = x.size()\n",
        "        out = x.view(*shape, last // self.pool_size, self.pool_size)\n",
        "        out, _ = out.max(-1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3b427ce7",
      "metadata": {
        "id": "3b427ce7"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        low_res_shape,\n",
        "        hidden_size=256,\n",
        "        embedding_dim=256,\n",
        "        checkpoint=None,\n",
        "        device=device,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes (int): Number of symbol classes\n",
        "            low_res_shape ((int, int, int)): Shape of the low resolution annotations\n",
        "                i.e. (C, W, H)\n",
        "            high_res_shape ((int, int, int)): Shape of the high resolution annotations\n",
        "                i.e. (C_prime, 2W, 2H)\n",
        "            hidden_size (int, optional): Hidden size of the GRU [Default: 256]\n",
        "            embedding_dim (int, optional): Dimension of the embedding [Default: 256]\n",
        "            checkpoint (dict, optional): State dictionary to be loaded\n",
        "            device (torch.device, optional): Device for the tensors\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        C = low_res_shape[1]\n",
        "\n",
        "        context_size = C\n",
        "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
        "        self.gru1 = nn.GRU(\n",
        "            input_size=embedding_dim, hidden_size=hidden_size, batch_first=True\n",
        "        )\n",
        "        self.gru2 = nn.GRU(\n",
        "            input_size=context_size, hidden_size=hidden_size, batch_first=True\n",
        "        )\n",
        "        # L = H * W\n",
        "        low_res_attn_size = low_res_shape[2] * low_res_shape[3]\n",
        "\n",
        "        self.coverage_attn_low = CoverageAttention(\n",
        "            C,\n",
        "            decoder_conv_filters,\n",
        "            attn_size=low_res_attn_size,\n",
        "            kernel_size=(11, 11),\n",
        "            padding=5,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.empty((num_classes, embedding_dim // 2)))\n",
        "        self.W_s = nn.Parameter(torch.empty((embedding_dim, hidden_size)))\n",
        "        self.W_c = nn.Parameter(torch.empty((embedding_dim, context_size)))\n",
        "        self.U_pred = nn.Parameter(torch.empty((n_prime, n)))\n",
        "        self.maxout = Maxout(2)\n",
        "        self.hidden_size = hidden_size\n",
        "        nn.init.xavier_normal_(self.W_o)\n",
        "        nn.init.xavier_normal_(self.W_s)\n",
        "        nn.init.xavier_normal_(self.W_c)\n",
        "        nn.init.xavier_normal_(self.U_pred)\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            self.load_state_dict(checkpoint)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros((1, batch_size, self.hidden_size))\n",
        "\n",
        "    def reset(self, batch_size):\n",
        "        self.coverage_attn_low.reset_alpha(batch_size)\n",
        "\n",
        "    # Unsqueeze and squeeze are used to add and remove the seq_len dimension,\n",
        "    # which is always 1 since only the previous symbol is provided, not a sequence.\n",
        "    # The inputs that are multiplied by the weights are transposed to get\n",
        "    # (m x batch_size) instead of (batch_size x m). The result of the\n",
        "    # multiplication is tranposed back.\n",
        "    def forward(self, x, hidden, low_res):\n",
        "        embedded = self.embedding(x)\n",
        "        pred, _ = self.gru1(embedded, hidden)\n",
        "        # u_pred is computed here instead of in the coverage attention, because the\n",
        "        # weight U_pred is shared and the coverage attention does not use pred for\n",
        "        # anything else. This avoids computing it twice.\n",
        "        u_pred = torch.matmul(self.U_pred, pred.squeeze(1).t()).t()\n",
        "        context_low = self.coverage_attn_low(low_res, u_pred)\n",
        "\n",
        "        context = context_low\n",
        "        new_hidden, _ = self.gru2(context.unsqueeze(1), pred.transpose(0, 1))\n",
        "        w_s = torch.matmul(self.W_s, new_hidden.squeeze(1).t()).t()\n",
        "        w_c = torch.matmul(self.W_c, context.t()).t()\n",
        "        out = embedded.squeeze(1) + w_s + w_c\n",
        "        out = self.maxout(out)\n",
        "        out = torch.matmul(self.W_o, out.t()).t()\n",
        "        return out, new_hidden.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "9ad9f7f5",
      "metadata": {
        "id": "9ad9f7f5"
      },
      "outputs": [],
      "source": [
        "dec = Decoder(112, low_res_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "d5f47bc0",
      "metadata": {
        "id": "d5f47bc0"
      },
      "outputs": [],
      "source": [
        "dec.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "f8da4926",
      "metadata": {
        "id": "f8da4926"
      },
      "outputs": [],
      "source": [
        "hidden = dec.init_hidden(10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "603bb546",
      "metadata": {
        "id": "603bb546"
      },
      "outputs": [],
      "source": [
        "expected = torch.zeros(10, 250, dtype=torch.long).to(device)\n",
        "batch_max_len = expected.size(1)\n",
        "use_teacher_forcing = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b4de7217",
      "metadata": {
        "id": "b4de7217"
      },
      "outputs": [],
      "source": [
        "sequence = torch.full(\n",
        "                (10, 1),\n",
        "                1,\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "previous = expected[:, 0] if use_teacher_forcing else sequence[:, -1]\n",
        "previous = previous.view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "6c3b1e8c",
      "metadata": {
        "id": "6c3b1e8c"
      },
      "outputs": [],
      "source": [
        "decoded_values = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "346840b9",
      "metadata": {
        "id": "346840b9"
      },
      "outputs": [],
      "source": [
        "out, hidden = dec(previous, hidden, low_res)\n",
        "hidden = hidden.detach()\n",
        "_, top1_id = torch.topk(out, 1)\n",
        "sequence = torch.cat((sequence, top1_id), dim=1)\n",
        "decoded_values.append(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68cc1ac",
      "metadata": {
        "id": "e68cc1ac"
      },
      "source": [
        "# Обучение нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "7f77a2d8",
      "metadata": {
        "id": "7f77a2d8"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import optim\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "5903dc18",
      "metadata": {
        "id": "5903dc18"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "09be731d",
      "metadata": {
        "id": "09be731d"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "num_epochs = 5\n",
        "print_epochs = 1\n",
        "learning_rate = 1e-3\n",
        "lr_epochs = 20\n",
        "lr_factor = 0.1\n",
        "weight_decay = 1e-4\n",
        "max_grad_norm = 5.0\n",
        "dropout_rate = 0.2\n",
        "teacher_forcing_ratio = 0.5\n",
        "seed = 1234"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "efaa4dea",
      "metadata": {
        "id": "efaa4dea"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                            drop_last=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            drop_last=False, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            drop_last=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "1848506c",
      "metadata": {
        "id": "1848506c"
      },
      "outputs": [],
      "source": [
        "# Number of bottlenecks\n",
        "num_bn = 2\n",
        "depth = 8\n",
        "multi_block_depth = depth // 2\n",
        "growth_rate = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "2e495db7",
      "metadata": {
        "id": "2e495db7"
      },
      "outputs": [],
      "source": [
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, input_size, growth_rate, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            growth_rate (int): Number of new features being added. That is the ouput\n",
        "                size of the last convolutional layer.\n",
        "            dropout_rate (float, optional): Probability of dropout [Default: 0.2]\n",
        "        \"\"\"\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        inter_size = num_bn * growth_rate\n",
        "        self.norm1 = nn.BatchNorm2d(input_size)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            input_size, inter_size, kernel_size=1, stride=1, bias=False\n",
        "        )\n",
        "        self.norm2 = nn.BatchNorm2d(inter_size)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            inter_size, growth_rate, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.relu(self.norm1(x)))\n",
        "        out = self.conv2(self.relu(self.norm2(out)))\n",
        "        out = self.dropout(out)\n",
        "        return torch.cat([x, out], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "037298f5",
      "metadata": {
        "id": "037298f5"
      },
      "outputs": [],
      "source": [
        "class TransitionBlock(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            output_size (int): Number of channels of the output\n",
        "        \"\"\"\n",
        "        super(TransitionBlock, self).__init__()\n",
        "        self.norm = nn.BatchNorm2d(input_size)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(\n",
        "            input_size, output_size, kernel_size=1, stride=1, bias=False\n",
        "        )\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.norm(x)))\n",
        "        return self.pool(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "75fa5ac7",
      "metadata": {
        "id": "75fa5ac7"
      },
      "outputs": [],
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Dense block\n",
        "\n",
        "    A dense block stacks several bottleneck blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, growth_rate, depth, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            growth_rate (int): Number of new features being added per bottleneck block\n",
        "            depth (int): Number of bottleneck blocks\n",
        "            dropout_rate (float, optional): Probability of dropout [Default: 0.2]\n",
        "        \"\"\"\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layers = [\n",
        "            BottleneckBlock(\n",
        "                input_size + i * growth_rate, growth_rate, dropout_rate=dropout_rate\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ]\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "da2aefee",
      "metadata": {
        "id": "da2aefee"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, img_channels=1, num_in_features=48, dropout_rate=0.2, checkpoint=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_channels (int, optional): Number of channels of the images [Default: 1]\n",
        "            num_in_features (int, optional): Number of channels that are created from\n",
        "                the input to feed to the first dense block [Default: 48]\n",
        "            dropout_rate (float, optional): Probability of dropout [Default: 0.2]\n",
        "            checkpoint (dict, optional): State dictionary to be loaded\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv0 = nn.Conv2d(\n",
        "            img_channels,\n",
        "            num_in_features,\n",
        "            kernel_size=7,\n",
        "            stride=2,\n",
        "            padding=3,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.norm0 = nn.BatchNorm2d(num_in_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        num_features = num_in_features\n",
        "        self.block1 = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        num_features = num_features + depth * growth_rate\n",
        "        self.trans1 = TransitionBlock(num_features, num_features // 2)\n",
        "        num_features = num_features // 2\n",
        "        self.block2 = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "\n",
        "        num_features = num_features + depth * growth_rate\n",
        "        self.trans2_norm = nn.BatchNorm2d(num_features)\n",
        "        self.trans2_relu = nn.ReLU(inplace=True)\n",
        "        self.trans2_conv = nn.Conv2d(\n",
        "            num_features, num_features // 2, kernel_size=1, stride=1, bias=False\n",
        "        )\n",
        "        self.trans2_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.multi_block = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=multi_block_depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        num_features = num_features // 2\n",
        "        self.block3 = DenseBlock(\n",
        "            num_features,\n",
        "            growth_rate=growth_rate,\n",
        "            depth=depth,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            self.load_state_dict(checkpoint)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv0(x)\n",
        "        out = self.relu(self.norm0(out))\n",
        "        out = self.max_pool(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.trans1(out)\n",
        "        out = self.block2(out)\n",
        "        out_before_trans2 = self.trans2_relu(self.trans2_norm(out))\n",
        "        out_A = self.trans2_conv(out_before_trans2)\n",
        "        out_A = self.trans2_pool(out_A)\n",
        "        out_A = self.block3(out_A)\n",
        "\n",
        "        return out_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "f45dab40",
      "metadata": {
        "id": "f45dab40"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(dropout_rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "6a3a1174",
      "metadata": {
        "id": "6a3a1174"
      },
      "outputs": [],
      "source": [
        "encoder.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b5f0b8f0",
      "metadata": {
        "id": "b5f0b8f0"
      },
      "outputs": [],
      "source": [
        "encoder1 = Encoder(dropout_rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "ec5ce52e",
      "metadata": {
        "id": "ec5ce52e"
      },
      "outputs": [],
      "source": [
        "sample = torch.zeros(batch_size, 1, 30, 100, device=device)\n",
        "low = encoder(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "2c508869",
      "metadata": {
        "id": "2c508869"
      },
      "outputs": [],
      "source": [
        "low_res_shape = low.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "1b9ee9fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b9ee9fe",
        "outputId": "4a31cc7a-c93f-4c77-8df7-743323ac8aba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 180, 1, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "low_res_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbe8e15",
      "metadata": {
        "id": "4bbe8e15"
      },
      "outputs": [],
      "source": [
        "n = 128\n",
        "n_prime = 256\n",
        "decoder_conv_filters = 64\n",
        "gru_hidden_size = 128\n",
        "embedding_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0b30e0",
      "metadata": {
        "id": "2e0b30e0"
      },
      "outputs": [],
      "source": [
        "class CoverageAttention(nn.Module):\n",
        "    # input_size = C\n",
        "    # output_size = q\n",
        "    # attn_size = L = H * W\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        output_size,\n",
        "        attn_size,\n",
        "        kernel_size,\n",
        "        padding=0,\n",
        "        device=device,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Number of channels of the input\n",
        "            output_size (int): Number of channels of the coverage\n",
        "            attn_size (int): Length of the annotation vector\n",
        "            kernel_size (int): Kernel size of the 1D convolutional layer\n",
        "            padding (int, optional): Padding of the 1D convolutional layer [Default: 0]\n",
        "            device (torch.device, optional): Device for the tensors\n",
        "        \"\"\"\n",
        "        super(CoverageAttention, self).__init__()\n",
        "        self.alpha = None\n",
        "        self.conv = nn.Conv2d(1, output_size, kernel_size=kernel_size, padding=padding)\n",
        "        self.U_a = nn.Parameter(torch.empty((n_prime, input_size)))\n",
        "        self.U_f = nn.Parameter(torch.empty((n_prime, output_size)))\n",
        "        self.nu_attn = nn.Parameter(torch.empty(n_prime))\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.attn_size = attn_size\n",
        "        self.device = device\n",
        "        nn.init.xavier_normal_(self.U_a)\n",
        "        nn.init.xavier_normal_(self.U_f)\n",
        "        # Xavier requires at least a 2D tensor.\n",
        "        nn.init.xavier_normal_(self.nu_attn.unsqueeze(0))\n",
        "\n",
        "    def reset_alpha(self, batch_size):\n",
        "        self.alpha = torch.zeros((batch_size, 1, self.attn_size), device=self.device)\n",
        "\n",
        "    def forward(self, x, u_pred):\n",
        "        batch_size = x.size(0)\n",
        "        if self.alpha is None:\n",
        "            self.reset_alpha(batch_size)\n",
        "        # Change the dimensions to make it possible to apply a 2D convolution\n",
        "        # From: (batch_size x L)\n",
        "        # To: (batch_size x H x W)\n",
        "        alpha_sum = self.alpha.sum(1).view(batch_size, x.size(2), x.size(3))\n",
        "        conv_out = self.conv(alpha_sum.unsqueeze(1))\n",
        "        # Change dimensions back\n",
        "        # From: (batch_size x output_size x H x W)\n",
        "        # To: (batch_size x output_size x L)\n",
        "        conv_out = conv_out.view(batch_size, self.output_size, -1)\n",
        "        # Change the dimensions\n",
        "        # From: (batch_size x C x H x W)\n",
        "        # To: (batch_size x C x L)\n",
        "        a = x.view(batch_size, x.size(1), -1)\n",
        "        u_a = torch.matmul(self.U_a, a)\n",
        "        u_f = torch.matmul(self.U_f, conv_out)\n",
        "        # u_pred is expanded from (batch_size x n_prime)\n",
        "        # to (batch_size x n_prime x L) because there are L components to which\n",
        "        # the same u_pred is added.\n",
        "        u_pred_expanded = u_pred.unsqueeze(2).expand_as(u_a)\n",
        "        tan_res = torch.tanh(u_pred_expanded + u_a + u_f)\n",
        "        e_t = torch.matmul(self.nu_attn, tan_res)\n",
        "        alpha_t = torch.softmax(e_t, dim=1)\n",
        "        self.alpha = torch.cat((self.alpha, alpha_t.detach().unsqueeze(1)), dim=1)\n",
        "        # alpha_t: (batch_size x L)\n",
        "        # a: (batch_size x C x L) but need (C x batch_size x L) for\n",
        "        # element-wise multiplication. So transpose them.\n",
        "        cA_t_L = alpha_t * a.transpose(0, 1)\n",
        "        # Transpose back\n",
        "        return cA_t_L.transpose(0, 1).sum(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d7809d",
      "metadata": {
        "id": "a7d7809d"
      },
      "outputs": [],
      "source": [
        "class Maxout(nn.Module):\n",
        "    def __init__(self, pool_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pool_size (int): Number of elements per pool\n",
        "        \"\"\"\n",
        "        super(Maxout, self).__init__()\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        [*shape, last] = x.size()\n",
        "        out = x.view(*shape, last // self.pool_size, self.pool_size)\n",
        "        out, _ = out.max(-1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "aab9f57c",
      "metadata": {
        "id": "aab9f57c"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        low_res_shape,\n",
        "        hidden_size=256,\n",
        "        embedding_dim=256,\n",
        "        checkpoint=None,\n",
        "        device=device,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes (int): Number of symbol classes\n",
        "            low_res_shape ((int, int, int)): Shape of the low resolution annotations\n",
        "                i.e. (C, W, H)\n",
        "            high_res_shape ((int, int, int)): Shape of the high resolution annotations\n",
        "                i.e. (C_prime, 2W, 2H)\n",
        "            hidden_size (int, optional): Hidden size of the GRU [Default: 256]\n",
        "            embedding_dim (int, optional): Dimension of the embedding [Default: 256]\n",
        "            checkpoint (dict, optional): State dictionary to be loaded\n",
        "            device (torch.device, optional): Device for the tensors\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        C = low_res_shape[1]\n",
        "\n",
        "        context_size = C\n",
        "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
        "        self.gru1 = nn.GRU(\n",
        "            input_size=embedding_dim, hidden_size=hidden_size, batch_first=True\n",
        "        )\n",
        "        self.gru2 = nn.GRU(\n",
        "            input_size=context_size, hidden_size=hidden_size, batch_first=True\n",
        "        )\n",
        "        # L = H * W\n",
        "        low_res_attn_size = low_res_shape[2] * low_res_shape[3]\n",
        "\n",
        "        self.coverage_attn_low = CoverageAttention(\n",
        "            C,\n",
        "            decoder_conv_filters,\n",
        "            attn_size=low_res_attn_size,\n",
        "            kernel_size=(11, 11),\n",
        "            padding=5,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.empty((num_classes, embedding_dim // 2)))\n",
        "        self.W_s = nn.Parameter(torch.empty((embedding_dim, hidden_size)))\n",
        "        self.W_c = nn.Parameter(torch.empty((embedding_dim, context_size)))\n",
        "        self.U_pred = nn.Parameter(torch.empty((n_prime, n)))\n",
        "        self.maxout = Maxout(2)\n",
        "        self.hidden_size = hidden_size\n",
        "        nn.init.xavier_normal_(self.W_o)\n",
        "        nn.init.xavier_normal_(self.W_s)\n",
        "        nn.init.xavier_normal_(self.W_c)\n",
        "        nn.init.xavier_normal_(self.U_pred)\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            self.load_state_dict(checkpoint)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros((1, batch_size, self.hidden_size))\n",
        "\n",
        "    def reset(self, batch_size):\n",
        "        self.coverage_attn_low.reset_alpha(batch_size)\n",
        "\n",
        "    # Unsqueeze and squeeze are used to add and remove the seq_len dimension,\n",
        "    # which is always 1 since only the previous symbol is provided, not a sequence.\n",
        "    # The inputs that are multiplied by the weights are transposed to get\n",
        "    # (m x batch_size) instead of (batch_size x m). The result of the\n",
        "    # multiplication is tranposed back.\n",
        "    def forward(self, x, hidden, low_res):\n",
        "        embedded = self.embedding(x)\n",
        "        pred, _ = self.gru1(embedded, hidden)\n",
        "        # u_pred is computed here instead of in the coverage attention, because the\n",
        "        # weight U_pred is shared and the coverage attention does not use pred for\n",
        "        # anything else. This avoids computing it twice.\n",
        "        u_pred = torch.matmul(self.U_pred, pred.squeeze(1).t()).t()\n",
        "        context_low = self.coverage_attn_low(low_res, u_pred)\n",
        "\n",
        "        context = context_low\n",
        "        new_hidden, _ = self.gru2(context.unsqueeze(1), pred.transpose(0, 1))\n",
        "        w_s = torch.matmul(self.W_s, new_hidden.squeeze(1).t()).t()\n",
        "        w_c = torch.matmul(self.W_c, context.t()).t()\n",
        "        out = embedded.squeeze(1) + w_s + w_c\n",
        "        out = self.maxout(out)\n",
        "        out = torch.matmul(self.W_o, out.t()).t()\n",
        "        return out, new_hidden.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "48994514",
      "metadata": {
        "id": "48994514"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(len(id_to_token), low_res_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83bd246",
      "metadata": {
        "id": "a83bd246"
      },
      "outputs": [],
      "source": [
        "decoder1 = Decoder(len(id_to_token), low_res_shape, high_res_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "085365a3",
      "metadata": {
        "id": "085365a3"
      },
      "outputs": [],
      "source": [
        "decoder.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "dd1f799f",
      "metadata": {
        "id": "dd1f799f"
      },
      "outputs": [],
      "source": [
        "encoder_params = [param for param in encoder.parameters() if param.requires_grad]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "5dfd37f2",
      "metadata": {
        "id": "5dfd37f2"
      },
      "outputs": [],
      "source": [
        "decoder_params = [param for param in decoder.parameters() if param.requires_grad]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "7ec3caae",
      "metadata": {
        "id": "7ec3caae"
      },
      "outputs": [],
      "source": [
        "params_to_optimise = [*encoder_params, *decoder_params]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "3822190a",
      "metadata": {
        "id": "3822190a"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "22fc57ea",
      "metadata": {
        "id": "22fc57ea"
      },
      "outputs": [],
      "source": [
        "optimiser = optim.Adadelta(\n",
        "        params_to_optimise, lr=learning_rate, weight_decay=weight_decay\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "07df8971",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07df8971",
        "outputId": "81e36225-09aa-44c2-94ce-5d838b40019b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63760"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "9d2582a3",
      "metadata": {
        "id": "9d2582a3"
      },
      "outputs": [],
      "source": [
        "def run_epoch(\n",
        "    data_loader,\n",
        "    enc,\n",
        "    dec,\n",
        "    epoch_text,\n",
        "    criterion,\n",
        "    optimiser,\n",
        "    teacher_forcing_ratio,\n",
        "    max_grad_norm,\n",
        "    device,\n",
        "    train=True,\n",
        "):\n",
        "    # Disables autograd during validation mode\n",
        "    torch.set_grad_enabled(train)\n",
        "    if train:\n",
        "        enc.train()\n",
        "        dec.train()\n",
        "    else:\n",
        "        enc.eval()\n",
        "        dec.eval()\n",
        "\n",
        "    losses = []\n",
        "    grad_norms = []\n",
        "    correct_symbols = 0\n",
        "    total_symbols = 0\n",
        "\n",
        "    covered_data = 0\n",
        "\n",
        "    with tqdm(\n",
        "        desc=\"{} ({})\".format(epoch_text, \"Train\" if train else \"Validation\"),\n",
        "        total=int(0.25*len(data_loader)),\n",
        "        dynamic_ncols=True,\n",
        "        leave=False,\n",
        "    ) as pbar:\n",
        "        for d in data_loader:\n",
        "            input, truth, expected = d\n",
        "            input = input.to(device)\n",
        "            # The last batch may not be a full batch\n",
        "            curr_batch_size = len(input)\n",
        "            expected = expected.to(device)\n",
        "            batch_max_len = expected.size(1)\n",
        "            # Replace -1 with the PAD token\n",
        "            expected[expected == -1] = token_to_id[PAD]\n",
        "            enc_low_res = enc(input)\n",
        "            # Decoder needs to be reset, because the coverage attention (alpha)\n",
        "            # only applies to the current image.\n",
        "            dec.reset(curr_batch_size)\n",
        "            hidden = dec.init_hidden(curr_batch_size).to(device)\n",
        "            # Starts with a START token\n",
        "            sequence = torch.full(\n",
        "                (curr_batch_size, 1),\n",
        "                token_to_id[START],\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "            # The teacher forcing is done per batch, not symbol\n",
        "            use_teacher_forcing = train and random.random() < teacher_forcing_ratio\n",
        "            decoded_values = []\n",
        "            for i in range(batch_max_len - 1):\n",
        "                previous = expected[:, i] if use_teacher_forcing else sequence[:, -1]\n",
        "                previous = previous.view(-1, 1)\n",
        "                out, hidden = dec(previous, hidden, enc_low_res)\n",
        "                hidden = hidden.detach()\n",
        "                _, top1_id = torch.topk(out, 1)\n",
        "                sequence = torch.cat((sequence, top1_id), dim=1)\n",
        "                decoded_values.append(out)\n",
        "\n",
        "            decoded_values = torch.stack(decoded_values, dim=2).to(device)\n",
        "            # decoded_values does not contain the start symbol\n",
        "            loss = criterion(decoded_values, expected[:, 1:])\n",
        "\n",
        "            if train:\n",
        "                optim_params = [\n",
        "                    p\n",
        "                    for param_group in optimiser.param_groups\n",
        "                    for p in param_group[\"params\"]\n",
        "                ]\n",
        "                optimiser.zero_grad()\n",
        "                loss.backward()\n",
        "                # Clip gradients, it returns the total norm of all parameters\n",
        "                grad_norm = nn.utils.clip_grad_norm_(\n",
        "                    optim_params, max_norm=max_grad_norm\n",
        "                )\n",
        "                grad_norms.append(grad_norm.detach().cpu())\n",
        "                optimiser.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            correct_symbols += torch.sum(sequence == expected, dim=(0, 1)).item()\n",
        "            total_symbols += expected.numel()\n",
        "            pbar.update(curr_batch_size)\n",
        "\n",
        "            covered_data += 2\n",
        "\n",
        "            if covered_data >= 0.25 * len(data_loader):\n",
        "                break\n",
        "\n",
        "    result = {\n",
        "        \"loss\": np.mean(losses),\n",
        "        \"correct_symbols\": correct_symbols,\n",
        "        \"total_symbols\": total_symbols,\n",
        "    }\n",
        "    if train:\n",
        "        result[\"grad_norm\"] = np.mean(grad_norms)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "630a2c8f",
      "metadata": {
        "id": "630a2c8f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "default_checkpoint = {\n",
        "    \"epoch\": 0,\n",
        "    \"train_losses\": [],\n",
        "    \"train_wer\": [],\n",
        "    \"validation_losses\": [],\n",
        "    \"validation_wer\": [],\n",
        "    \"test_losses\": [],\n",
        "    \"test_wer\": [],\n",
        "    \"lr\": [],\n",
        "    \"grad_norm\": [],\n",
        "    \"model\": {},\n",
        "}\n",
        "\n",
        "\n",
        "def save_checkpoint(checkpoint, dir=\"./checkpoints\", prefix=\"\"):\n",
        "    # Padded to 4 digits because of lexical sorting of numbers.\n",
        "    # e.g. 0009.pth\n",
        "    filename = \"{prefix}{num:0>4}.pth\".format(num=checkpoint[\"epoch\"], prefix=prefix)\n",
        "    if not os.path.exists(dir):\n",
        "        os.mkdir(dir)\n",
        "    torch.save(checkpoint, os.path.join(dir, filename))\n",
        "\n",
        "\n",
        "def load_checkpoint(path, cuda=use_cuda):\n",
        "    if cuda:\n",
        "        return torch.load(path)\n",
        "    else:\n",
        "        # Load GPU model on CPU\n",
        "        return torch.load(path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "\n",
        "def init_tensorboard(name=\"\", base_dir=\"./tensorboard\"):\n",
        "    return SummaryWriter(os.path.join(base_dir, name))\n",
        "\n",
        "\n",
        "def write_tensorboard(\n",
        "    writer,\n",
        "    epoch,\n",
        "    grad_norm,\n",
        "    train_loss,\n",
        "    train_wer,\n",
        "    validation_loss,\n",
        "    validation_wer,\n",
        "    test_loss,\n",
        "    test_wer,\n",
        "    encoder,\n",
        "    decoder,\n",
        "):\n",
        "    writer.add_scalar(\"train_loss\", train_loss, epoch)\n",
        "    writer.add_scalar(\"train_wer\", train_wer, epoch)\n",
        "    writer.add_scalar(\"validation_loss\", validation_loss, epoch)\n",
        "    writer.add_scalar(\"validation_wer\", validation_wer, epoch)\n",
        "    writer.add_scalar(\"test_loss\", test_loss, epoch)\n",
        "    writer.add_scalar(\"test_wer\", test_wer, epoch)\n",
        "    writer.add_scalar(\"grad_norm\", grad_norm, epoch)\n",
        "\n",
        "    for name, param in encoder.named_parameters():\n",
        "        writer.add_histogram(\n",
        "            \"encoder/{}\".format(name), param.detach().cpu().numpy(), epoch\n",
        "        )\n",
        "        if param.grad is not None:\n",
        "            writer.add_histogram(\n",
        "                \"encoder/{}/grad\".format(name), param.grad.detach().cpu().numpy(), epoch\n",
        "            )\n",
        "\n",
        "    for name, param in decoder.named_parameters():\n",
        "        writer.add_histogram(\n",
        "            \"decoder/{}\".format(name), param.detach().cpu().numpy(), epoch\n",
        "        )\n",
        "        if param.grad is not None:\n",
        "            writer.add_histogram(\n",
        "                \"decoder/{}/grad\".format(name), param.grad.detach().cpu().numpy(), epoch\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "1639cb4d",
      "metadata": {
        "id": "1639cb4d"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    enc,\n",
        "    dec,\n",
        "    optimiser,\n",
        "    criterion,\n",
        "    train_data_loader,\n",
        "    validation_data_loader,\n",
        "    test_data_loader,\n",
        "    device,\n",
        "    teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "    lr_scheduler=None,\n",
        "    num_epochs=4,\n",
        "    print_epochs=None,\n",
        "    checkpoint=default_checkpoint,\n",
        "    prefix=\"\",\n",
        "    max_grad_norm=max_grad_norm,\n",
        "):\n",
        "    if print_epochs is None:\n",
        "        print_epochs = num_epochs\n",
        "\n",
        "    writer = init_tensorboard(name=prefix.strip(\"-\"))\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "    train_accuracy = checkpoint[\"train_wer\"]\n",
        "    train_losses = checkpoint[\"train_losses\"]\n",
        "    validation_accuracy = checkpoint[\"validation_wer\"]\n",
        "    validation_losses = checkpoint[\"validation_losses\"]\n",
        "    test_accuracy = checkpoint[\"test_wer\"]\n",
        "    test_losses = checkpoint[\"test_losses\"]\n",
        "    learning_rates = checkpoint[\"lr\"]\n",
        "    grad_norms = checkpoint[\"grad_norm\"]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        if lr_scheduler:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        epoch_text = \"[{current:>{pad}}/{end}] Epoch {epoch}\".format(\n",
        "            current=epoch + 1,\n",
        "            end=num_epochs,\n",
        "            epoch=start_epoch + epoch + 1,\n",
        "            pad=len(str(num_epochs)),\n",
        "        )\n",
        "\n",
        "        train_result = run_epoch(\n",
        "            train_data_loader,\n",
        "            enc,\n",
        "            dec,\n",
        "            epoch_text,\n",
        "            criterion,\n",
        "            optimiser,\n",
        "            teacher_forcing_ratio,\n",
        "            max_grad_norm,\n",
        "            device,\n",
        "            train=True,\n",
        "        )\n",
        "        train_losses.append(train_result[\"loss\"])\n",
        "        grad_norms.append(train_result[\"grad_norm\"])\n",
        "        train_epoch_accuracy = (\n",
        "            train_result[\"correct_symbols\"] / train_result[\"total_symbols\"]\n",
        "        )\n",
        "        train_accuracy.append(train_epoch_accuracy)\n",
        "        epoch_lr = lr_scheduler.get_lr()[0]\n",
        "        learning_rates.append(epoch_lr)\n",
        "\n",
        "        validation_result = run_epoch(\n",
        "            validation_data_loader,\n",
        "            enc,\n",
        "            dec,\n",
        "            epoch_text,\n",
        "            criterion,\n",
        "            optimiser,\n",
        "            teacher_forcing_ratio,\n",
        "            max_grad_norm,\n",
        "            device,\n",
        "            train=False,\n",
        "        )\n",
        "        validation_losses.append(validation_result[\"loss\"])\n",
        "        validation_epoch_accuracy = (\n",
        "            validation_result[\"correct_symbols\"] / validation_result[\"total_symbols\"]\n",
        "        )\n",
        "        validation_accuracy.append(validation_epoch_accuracy)\n",
        "\n",
        "        test_result = run_epoch(\n",
        "            test_data_loader,\n",
        "            enc,\n",
        "            dec,\n",
        "            epoch_text,\n",
        "            criterion,\n",
        "            optimiser,\n",
        "            teacher_forcing_ratio,\n",
        "            max_grad_norm,\n",
        "            device,\n",
        "            train=False,\n",
        "        )\n",
        "        test_losses.append(test_result[\"loss\"])\n",
        "        test_epoch_accuracy = (\n",
        "            test_result[\"correct_symbols\"] / test_result[\"total_symbols\"]\n",
        "        )\n",
        "        test_accuracy.append(test_epoch_accuracy)\n",
        "\n",
        "        save_checkpoint(\n",
        "            {\n",
        "                \"epoch\": start_epoch + epoch + 1,\n",
        "                \"train_losses\": train_losses,\n",
        "                \"train_wer\": train_accuracy,\n",
        "                \"validation_losses\": validation_losses,\n",
        "                \"validation_wer\": validation_accuracy,\n",
        "                \"lr\": learning_rates,\n",
        "                \"grad_norm\": grad_norms,\n",
        "                \"model\": {\"encoder\": enc.state_dict(), \"decoder\": dec.state_dict()},\n",
        "                \"optimiser\": optimiser.state_dict(),\n",
        "            },\n",
        "            prefix=prefix,\n",
        "        )\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "        if epoch % print_epochs == 0 or epoch == num_epochs - 1:\n",
        "            print(\n",
        "                (\n",
        "                    \"{epoch_text}: \"\n",
        "                    \"Train WER = {train_accuracy:.5f}, \"\n",
        "                    \"Train Loss = {train_loss:.5f}, \"\n",
        "                    \"Validation WER = {validation_accuracy:.5f}, \"\n",
        "                    \"Validation Loss = {validation_loss:.5f}, \"\n",
        "                    \"Test WER = {validation_accuracy:.5f}, \"\n",
        "                    \"Test Loss = {validation_loss:.5f}, \"\n",
        "                    \"lr = {lr} \"\n",
        "                    \"(time elapsed {time})\"\n",
        "                ).format(\n",
        "                    epoch_text=epoch_text,\n",
        "                    train_accuracy=train_epoch_accuracy,\n",
        "                    train_loss=train_result[\"loss\"],\n",
        "                    validation_accuracy=validation_epoch_accuracy,\n",
        "                    validation_loss=validation_result[\"loss\"],\n",
        "                    test_accuracy=test_epoch_accuracy,\n",
        "                    test_loss=test_result[\"loss\"],\n",
        "                    lr=epoch_lr,\n",
        "                    time=elapsed_time,\n",
        "                )\n",
        "            )\n",
        "            write_tensorboard(\n",
        "                writer,\n",
        "                start_epoch + epoch + 1,\n",
        "                train_result[\"grad_norm\"],\n",
        "                train_result[\"loss\"],\n",
        "                train_epoch_accuracy,\n",
        "                validation_result[\"loss\"],\n",
        "                validation_epoch_accuracy,\n",
        "                test_result[\"loss\"],\n",
        "                test_epoch_accuracy,\n",
        "                enc,\n",
        "                dec,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "f518f313",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "f518f313",
        "outputId": "396c6f55-997a-427a-a444-dd00d4827f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-dbf563ae7eaa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run_epoch(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-e97b62542fbd>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_loader, enc, dec, epoch_text, criterion, optimiser, teacher_forcing_ratio, max_grad_norm, device, train)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mprevious\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprevious\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_low_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-34da14551f89>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden, low_res)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_low\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mw_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mw_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             result = _VF.gru(\n\u001b[0m\u001b[1;32m   1393\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run_epoch(\n",
        "    train_dataloader,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    \"\",\n",
        "    criterion,\n",
        "    optimiser,\n",
        "    teacher_forcing_ratio,\n",
        "    max_grad_norm,\n",
        "    device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ebacec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ebacec",
        "outputId": "d32de262-c502-4683-8e77-6c3d1980923e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:536: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  _warn_get_lr_called_within_step(self)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/5] Epoch 1: Train WER = 0.19297, Train Loss = 4.75779, Validation WER = 0.23292, Validation Loss = 4.37214, Test WER = 0.23292, Test Loss = 4.37214, lr = 0.001 (time elapsed 01:36:31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2/5] Epoch 2: Train WER = 0.27658, Train Loss = 3.85462, Validation WER = 0.24545, Validation Loss = 4.11004, Test WER = 0.24545, Test Loss = 4.11004, lr = 0.001 (time elapsed 01:35:49)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[3/5] Epoch 3 (Train):  28%|██▊       | 4460/15940 [25:09<1:16:12,  2.51it/s]"
          ]
        }
      ],
      "source": [
        "lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimiser, step_size=lr_epochs, gamma=lr_factor\n",
        "    )\n",
        "\n",
        "train(\n",
        "    encoder,\n",
        "    decoder,\n",
        "    optimiser,\n",
        "    criterion,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    test_dataloader,\n",
        "    teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    print_epochs=print_epochs,\n",
        "    device=device,\n",
        "    num_epochs=num_epochs,\n",
        "    checkpoint=default_checkpoint,\n",
        "    prefix=\"-\",\n",
        "    max_grad_norm=max_grad_norm,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3660d57b",
      "metadata": {
        "id": "3660d57b"
      },
      "source": [
        "# Анализ результатов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce10108",
      "metadata": {
        "id": "7ce10108"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "44e8a824",
      "metadata": {
        "id": "44e8a824"
      },
      "source": [
        "# Выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02166ce",
      "metadata": {
        "id": "d02166ce"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}